{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c7fadd-b9f1-4039-a066-a62cdfa07171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 00:20:55.669026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 00:20:55.851133: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-01 00:20:57.542087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/software/software/ZeroMQ/4.3.4-GCCcore-11.2.0/lib:/opt/software/software/util-linux/2.37-GCCcore-11.2.0/lib:/opt/software/software/libsodium/1.0.18-GCCcore-11.2.0/lib:/opt/software/software/OpenPGM/5.2.122-GCCcore-11.2.0/lib:/opt/software/software/Python/3.9.6-GCCcore-11.2.0/lib:/opt/software/software/OpenSSL/1.1/lib:/opt/software/software/libffi/3.4.2-GCCcore-11.2.0/lib64:/opt/software/software/GMP/6.2.1-GCCcore-11.2.0/lib:/opt/software/software/XZ/5.2.5-GCCcore-11.2.0/lib:/opt/software/software/SQLite/3.36-GCCcore-11.2.0/lib:/opt/software/software/Tcl/8.6.11-GCCcore-11.2.0/lib:/opt/software/software/libreadline/8.1-GCCcore-11.2.0/lib:/opt/software/software/ncurses/6.2-GCCcore-11.2.0/lib:/opt/software/software/bzip2/1.0.8-GCCcore-11.2.0/lib:/opt/software/software/binutils/2.37-GCCcore-11.2.0/lib:/opt/software/software/zlib/1.2.11-GCCcore-11.2.0/lib:/opt/software/software/GCCcore/11.2.0/lib64\n",
      "2022-12-01 00:20:57.543593: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/software/software/ZeroMQ/4.3.4-GCCcore-11.2.0/lib:/opt/software/software/util-linux/2.37-GCCcore-11.2.0/lib:/opt/software/software/libsodium/1.0.18-GCCcore-11.2.0/lib:/opt/software/software/OpenPGM/5.2.122-GCCcore-11.2.0/lib:/opt/software/software/Python/3.9.6-GCCcore-11.2.0/lib:/opt/software/software/OpenSSL/1.1/lib:/opt/software/software/libffi/3.4.2-GCCcore-11.2.0/lib64:/opt/software/software/GMP/6.2.1-GCCcore-11.2.0/lib:/opt/software/software/XZ/5.2.5-GCCcore-11.2.0/lib:/opt/software/software/SQLite/3.36-GCCcore-11.2.0/lib:/opt/software/software/Tcl/8.6.11-GCCcore-11.2.0/lib:/opt/software/software/libreadline/8.1-GCCcore-11.2.0/lib:/opt/software/software/ncurses/6.2-GCCcore-11.2.0/lib:/opt/software/software/bzip2/1.0.8-GCCcore-11.2.0/lib:/opt/software/software/binutils/2.37-GCCcore-11.2.0/lib:/opt/software/software/zlib/1.2.11-GCCcore-11.2.0/lib:/opt/software/software/GCCcore/11.2.0/lib64\n",
      "2022-12-01 00:20:57.543615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f43300f-ce21-4ce1-bac4-dbc6df746fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2022 00:21:22 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21e5c5-2a8b-44e7-90b0-34bfbdc58593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"jfleg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "649fef46-ab1c-452e-9bad-63e7dbc044aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
    "eval_dataset = load_dataset(\"jfleg\", split='test[248:]')\n",
    "train_dataset2 = load_dataset(\"jfleg\", split='test[:248]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efdc3b9-164c-4fda-a793-a073ff699487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'corrections'],\n",
      "    num_rows: 755\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d551d2-b3a6-4c46-ac7d-6d91f0760465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'corrections'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55d2939-6a69-4ff0-8a05-6f43dfa88f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'corrections'],\n",
      "    num_rows: 248\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e51870b-b201-4c65-9225-5fbceea22735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ', 'So I think we could not live if older people did not develop science and technologies . ', 'So I think we can not live if old people could not find science and technologies and they did not develop . ', 'So I think we can not live if old people can not find the science and technology that has not been developed . ']\n",
      "So I think we would not be alive if our ancestors did not develop sciences and technologies . \n",
      "['Not for use with a car . ', 'Do not use in the car . ', 'Car not for use . ', 'Can not use the car . ']\n",
      "Not for use with a car . \n"
     ]
    }
   ],
   "source": [
    "for case in train_dataset[\"corrections\"][:2]:\n",
    "  print(case)\n",
    "  print(case[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4871e6e-9de3-4c6e-a3d4-0da404f4cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "  (\" .\", \".\"), \n",
    "  (\" ,\", \",\"),\n",
    "  (\" '\", \"'\"),\n",
    "  (\" ?\", \"?\"),\n",
    "  (\" !\", \"!\"),\n",
    "  (\" :\", \"!\"),\n",
    "  (\" ;\", \"!\"),\n",
    "  (\" n't\", \"n't\"),\n",
    "  (\" v\", \"n't\"),\n",
    "  (\"2 0 0 6\", \"2006\"),\n",
    "  (\"5 5\", \"55\"),\n",
    "  (\"4 0 0\", \"400\"),\n",
    "  (\"1 7-5 0\", \"1750\"),\n",
    "  (\"2 0 %\", \"20%\"),\n",
    "  (\"5 0\", \"50\"),\n",
    "  (\"1 2\", \"12\"),\n",
    "  (\"1 0\", \"10\"),\n",
    "  ('\" ballast water', '\"ballast water')\n",
    "]\n",
    "\n",
    "def remove_excess_spaces(text):\n",
    "  for rep in replacements:\n",
    "    text = text.replace(rep[0], rep[1])\n",
    "\n",
    "  return text\n",
    "def generate_csv(csv_path, dataset):\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writter = csv.writer(csvfile)\n",
    "        writter.writerow([\"input\", \"target\"])\n",
    "        for case in dataset:\n",
    "            input_text = case[\"sentence\"]\n",
    "            for correction in case[\"corrections\"]:\n",
    "                if input_text and correction:\n",
    "                    input_text = remove_excess_spaces(input_text)\n",
    "                    correction = remove_excess_spaces(correction)\n",
    "                    writter.writerow([input_text, correction])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ecea07-42e6-40f6-ba8c-a22cbee59a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_csv(\"train.csv\", train_dataset)\n",
    "generate_csv(\"eval.csv\", eval_dataset)\n",
    "generate_csv(\"train2.csv\", train_dataset2)\n",
    "import pandas as pd\n",
    "df = pd.concat(map(pd.read_csv, ['train.csv', 'train2.csv']), ignore_index=True)\n",
    "df.to_csv('new_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419963e3-94e8-49a9-a916-8919401a9477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2022 00:21:35 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/p793x363/.cache/huggingface/datasets/csv/default-d98a1db4dd444943/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de9e0e3fc1d4d20831565cc0fd2b178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ceb7f3cb9a4ddd94fb46a0533d9ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/p793x363/.cache/huggingface/datasets/csv/default-d98a1db4dd444943/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a916707aba42feb82bd7a4216ce49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57b73e5a5414e8087fa75b30a6b9762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p793x363/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1996\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1996' max='1996' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1996/1996 04:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "before_result = happy_tt.eval(\"eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf414f3e-c0a3-496a-a685-63a1c5fecb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loss:  1.5075130462646484\n"
     ]
    }
   ],
   "source": [
    "print(\"Before loss: \", before_result.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31294d37-c29f-489b-a9ff-21c3daebdeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import TTTrainArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "002cb2a4-d4ec-4e9f-b06f-5735745dcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TTTrainArgs(batch_size=8,num_train_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26cf09ac-ab13-4b42-a296-591983712293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2022 00:25:42 - INFO - happytransformer.happy_transformer -   Preprocessing training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/p793x363/.cache/huggingface/datasets/csv/default-3dbc681b41eff753/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcac13aaa4c94f629f892d6c4ae3f9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc9f911a4a34f14b9e9b95673657923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/p793x363/.cache/huggingface/datasets/csv/default-3dbc681b41eff753/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b961eda0a38f4627bd9f7d7b3a9a2ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4427a43102948d69c913de5b41b6f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2022 00:25:46 - INFO - happytransformer.happy_transformer -   Training...\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 4008\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5010' max='5010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5010/5010 1:45:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.258600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "happy_tt.train(\"new_train.csv\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad57e564-7d8e-4e3b-841a-cfc56dc649a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2022 02:11:42 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d208800434454800ad979c5df9ace271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e008194b5a42b7873a38bd98d73f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1996\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1996' max='1996' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1996/1996 07:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "after_result = happy_tt.eval(\"eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "885a235f-e37b-41bd-95ab-7732403b94e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loss: 0.490283727645874\n"
     ]
    }
   ],
   "source": [
    "print(\"After loss:\", after_result.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39a9701-3e54-4199-b18e-e5a747c29ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Model_gec2/config.json\n",
      "Model weights saved in Model_gec2/pytorch_model.bin\n",
      "tokenizer config file saved in Model_gec2/tokenizer_config.json\n",
      "Special tokens file saved in Model_gec2/special_tokens_map.json\n",
      "Copy vocab file to Model_gec2/spiece.model\n"
     ]
    }
   ],
   "source": [
    "happy_tt.save(\"Model_gec/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ccc9c7-7916-487b-b1ff-aad8627f9a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2022 08:35:48 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is moving here.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText\n",
    "from happytransformer import TTSettings\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\",load_path='Model_gec')\n",
    "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=20)\n",
    "text = 'He are moving here.'\n",
    "result = happy_tt.generate_text(text, args=beam_settings)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e195fb-c72e-4a71-893c-586606b3cf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
